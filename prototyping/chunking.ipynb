{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: p:\\Dev\\hackaTum\\loganalysis\\tutorial\n",
      "3642936\n",
      "1000\n",
      "Nov 09 13:11:41 CMX50070-101776 health_service[555]: removing /var/log/coredump/core.CMXmarsServer.1000.b98ef10ab3f8471aa548add9b9e5d223.86893.1699426915000000.lz4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "print(\"Current Working Directory:\", current_directory)\n",
    "\n",
    "with open('../data/test_log1.out', 'r') as file:\n",
    "    context = file.read()\n",
    "\n",
    "print(len(context))\n",
    "contextLines = context.splitlines()\n",
    "\n",
    "contextLines = contextLines[2000:3000]\n",
    "print(len(contextLines))\n",
    "print(contextLines[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    while start < len(context):\\n        end = min(start + max_length, len(context))\\n        chunks.append(context[start:end])\\n        if end == len(context):\\n            break\\n        start = end - overlap\\n    return chunks\\n'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_context(context, overlap=50, max_length=512):\n",
    "    \"\"\"\n",
    "    Function for splitting context into overlapping chunks.\n",
    "    \n",
    "    param context: This is the text that you want to split into chunks. \n",
    "    The function will split this text based on the max_length and overlap parameters.\n",
    "\n",
    "    param overlap (default=50): This is the number of characters that will overlap between each chunk. \n",
    "    This is used to ensure that the context is not cut off in the middle of a sentence, which could make the text difficult to understand.\n",
    "\n",
    "    param max_length (default=512): This is the maximum length of each chunk. \n",
    "    The function will split the context into chunks of this length, with the exception of the last chunk, which may be shorter.\n",
    "\n",
    "    The function returns a list of chunks, where each chunk is a string of text from the context. \n",
    "    The chunks are created by starting at the beginning of the context and moving forward max_length\n",
    "    characters at a time, with an overlap of overlap characters between each chunk.\n",
    "    \"\"\"\n",
    "    \n",
    "    chunks = []\n",
    "    start = 0\n",
    "    maxChunkLength = 512\n",
    "    maxLineLength = max(contextLines, key=len)\n",
    "    shortLines = list(filter(lambda line: len(line) <= maxChunkLength, contextLines))\n",
    "    print(\"lines with less than 513 chars: \", len(list(shortLines)))\n",
    "    print(\"maxLineLength: \" , (maxLineLength))\n",
    "\n",
    "    while start < len(shortLines):\n",
    "        chunk = \"\"\n",
    "        \n",
    "        # you need the -2 here, because 'n\\' gets added to the chunk as well'\n",
    "        while len(chunk) <= maxChunkLength-2 and start < len(shortLines):\n",
    "            if(len(chunk) + len(shortLines[start])) <= maxChunkLength-2:\n",
    "                chunk += shortLines[start]\n",
    "                chunk += '\\n'\n",
    "                start += 1\n",
    "            else:\n",
    "                break\n",
    "        chunks.append(chunk)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_error_chunks(chunks):\n",
    "    keywords = [\"error\", \"failure\", \"warning\", \"not found\", \"missing\", \"problem\"]\n",
    "    return [chunk for chunk in chunks if any(keyword in chunk for keyword in keywords)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linesUnder513 1000\n",
      "maxLineLength:  Nov 09 13:11:48 CMX50070-101776 rs_callysto.xlapi_nb_container_settings[3036]: [info     ] Write .env file for XlapiContainerSettings(ip='*', port=7777, base_url='/notebooks', token='instrument', xlapi_instrument_address='localhost') to /run/user/0/mrt.callysto/.env.prod context=_setup | start pathname=/usr/lib/callysto/venv/lib/python3.10/site-packages/rs_callysto/xlapi_nb_container_settings.py lineno=46\n",
      "lenSplit:  492\n",
      "301\n",
      "filteredContext length:  13\n",
      "Nov 09 13:11:42 CMX50070-101776 systemd[1859]: mrt.base.service: Executable /usr/local/LoggingService/bin/stopcurrentlogger.sh missing, skipping: Permission denied\n",
      "Nov 09 13:11:42 CMX50070-101776 systemd[1]: Started MRT Base Software.\n",
      "Nov 09 13:11:42 CMX50070-101776 systemd[1]: Started Battery Measurement Service.\n",
      "Nov 09 13:11:42 CMX50070-101776 systemd[1]: Starting MRT Deployment Calculation Service...\n",
      "\n",
      "----------\n",
      "Nov 09 13:11:42 CMX50070-101776 Xserver[1884]:         Before reporting problems, check http://wiki.x.org\n",
      "Nov 09 13:11:42 CMX50070-101776 Xserver[1884]:         to make sure that you have the latest version.\n",
      "Nov 09 13:11:42 CMX50070-101776 Xserver[1884]: Markers: (--) probed, (**) from config file, (==) default setting,\n",
      "Nov 09 13:11:42 CMX50070-101776 Xserver[1884]:         (++) from command line, (!!) notice, (II) informational,\n",
      "\n",
      "----------\n",
      "Nov 09 13:11:42 CMX50070-101776 Xserver[1884]:         (WW) warning, (EE) error, (NI) not implemented, (??) unknown.\n",
      "Nov 09 13:11:42 CMX50070-101776 Xserver[1884]: (==) Log file: \"/var/log/Xorg.0.log\", Time: Thu Nov  9 13:11:42 2023\n",
      "Nov 09 13:11:42 CMX50070-101776 Xserver[1884]: (==) Using config directory: \"/etc/X11/xorg.conf.d\"\n",
      "Nov 09 13:11:42 CMX50070-101776 Xserver[1884]: (==) Using system config directory \"/usr/share/X11/xorg.conf.d\"\n",
      "\n",
      "----------\n",
      "Nov 09 13:11:43 CMX50070-101776 sshd[2446]: error: kex_exchange_identification: Connection closed by remote host\n",
      "Nov 09 13:11:43 CMX50070-101776 sshd[2446]: Connection closed by 10.0.54.73 port 48154\n",
      "Nov 09 13:11:43 CMX50070-101776 systemd[1]: sshd@0-10.102.37.173:22-10.0.54.73:48154.service: Deactivated successfully.\n",
      "Nov 09 13:11:43 CMX50070-101776 node[405]: Created application facet succesfully\n",
      "\n",
      "----------\n",
      "Nov 09 13:11:43 CMX50070-101776 systemd[1]: Started OpenSSH Per-Connection Daemon (172.24.26.36:58080).\n",
      "Nov 09 13:11:43 CMX50070-101776 sshd[2542]: error: kex_exchange_identification: Connection closed by remote host\n",
      "Nov 09 13:11:43 CMX50070-101776 sshd[2542]: Connection closed by 172.24.26.36 port 58080\n",
      "Nov 09 13:11:43 CMX50070-101776 systemd[1]: sshd@1-10.102.37.173:22-172.24.26.36:58080.service: Deactivated successfully.\n",
      "Nov 09 13:11:43 CMX50070-101776 node[405]: Get index for mrt.nrmmwue-helpk70...\n",
      "\n",
      "----------\n",
      "Nov 09 13:11:44 CMX50070-101776 dockerd[1853]: time=\"2023-11-09T13:11:44.902972912Z\" level=info msg=\"[graphdriver] using prior storage driver: overlay2\"\n",
      "Nov 09 13:11:44 CMX50070-101776 Xserver[2286]: libGL error: MESA-LOADER: failed to open radeonsi: /usr/lib/dri/radeonsi_dri.so: cannot open shared object file: No such file or directory (search paths /usr/lib/dri, suffix _dri)\n",
      "Nov 09 13:11:44 CMX50070-101776 Xserver[2286]: libGL error: failed to load driver: radeonsi\n",
      "\n",
      "----------\n",
      "Nov 09 13:11:44 CMX50070-101776 kernel: CMX_AU_IOCTL_SET_CLEAR_SIG_FPGA call\n",
      "Nov 09 13:11:44 CMX50070-101776 kernel: CMX_IOCTL_GET_SLOT_ID call\n",
      "Nov 09 13:11:44 CMX50070-101776 Xserver[2286]: libGL error: failed to open /dev/dri/card0: No such file or directory\n",
      "Nov 09 13:11:44 CMX50070-101776 Xserver[2286]: libGL error: failed to load driver: radeonsi\n",
      "Nov 09 13:11:44 CMX50070-101776 rngd[407]: [jitter]: Enabling JITTER rng support\n",
      "Nov 09 13:11:44 CMX50070-101776 rngd[407]: [jitter]: Initialized\n",
      "\n",
      "----------\n",
      "Nov 09 13:11:45 CMX50070-101776 dockerd[1853]: time=\"2023-11-09T13:11:45.067562294Z\" level=warning msg=\"Your kernel does not support cgroup blkio weight\"\n",
      "Nov 09 13:11:45 CMX50070-101776 dockerd[1853]: time=\"2023-11-09T13:11:45.067596248Z\" level=warning msg=\"Your kernel does not support cgroup blkio weight_device\"\n",
      "Nov 09 13:11:45 CMX50070-101776 dockerd[1853]: time=\"2023-11-09T13:11:45.067605265Z\" level=warning msg=\"Your kernel does not support cgroup blkio throttle.read_bps_device\"\n",
      "\n",
      "----------\n",
      "Nov 09 13:11:45 CMX50070-101776 dockerd[1853]: time=\"2023-11-09T13:11:45.067612849Z\" level=warning msg=\"Your kernel does not support cgroup blkio throttle.write_bps_device\"\n",
      "Nov 09 13:11:45 CMX50070-101776 dockerd[1853]: time=\"2023-11-09T13:11:45.067621135Z\" level=warning msg=\"Your kernel does not support cgroup blkio throttle.read_iops_device\"\n",
      "\n",
      "----------\n",
      "Nov 09 13:11:45 CMX50070-101776 dockerd[1853]: time=\"2023-11-09T13:11:45.067628810Z\" level=warning msg=\"Your kernel does not support cgroup blkio throttle.write_iops_device\"\n",
      "Nov 09 13:11:45 CMX50070-101776 dockerd[1853]: time=\"2023-11-09T13:11:45.067827022Z\" level=info msg=\"Loading containers: start.\"\n",
      "\n",
      "----------\n",
      "Nov 09 13:11:45 CMX50070-101776 audit: PROCTITLE proctitle=2F7573722F7362696E2F69707461626C6573002D2D77616974002D74006E6174002D4900504F5354524F5554494E47002D73003130302E38322E36342E302F32350000002D6F00646F636B657230002D6A004D415351554552414445\n",
      "Nov 09 13:11:45 CMX50070-101776 Xserver[2286]: libEGL warning: DRI2: could not open /dev/dri/card0 (No such file or directory)\n",
      "Nov 09 13:11:45 CMX50070-101776 audit[2747]: NETFILTER_CFG table=nat family=2 entries=8 op=xt_replace pid=2747 comm=\"iptables\"\n",
      "\n",
      "----------\n",
      "Nov 09 13:11:45 CMX50070-101776 xu_launcher[2941]: ./o_upper_20.bmp\n",
      "Nov 09 13:11:45 CMX50070-101776 xu_launcher[2941]: ./six_12.bmp\n",
      "Nov 09 13:11:45 CMX50070-101776 xu_launcher[2941]: ./g_lower_12.bmp\n",
      "Nov 09 13:11:45 CMX50070-101776 xu_launcher[2941]: ./warning_mark.bmp\n",
      "Nov 09 13:11:45 CMX50070-101776 xu_launcher[2941]: ./t_lower_12.bmp\n",
      "Nov 09 13:11:45 CMX50070-101776 avahi-daemon[379]: Interface docker0.IPv4 no longer relevant for mDNS.\n",
      "\n",
      "----------\n",
      "Nov 09 13:11:48 CMX50070-101776 rs_callysto.callysto_server._logging_configuration[3036]: [info     ] write json log to /var/log/callysto.log.json type=json log_level=20 filename=/var/log/callysto.log.json pathname=/usr/lib/callysto/venv/lib/python3.10/site-packages/rs_callysto/callysto_server/_logging_configuration.py lineno=183\n",
      "Nov 09 13:11:48 CMX50070-101776 bash[3046]: Removing network cmxreporting_default\n",
      "Nov 09 13:11:48 CMX50070-101776 bash[3046]: Network cmxreporting_default not found.\n",
      "\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "splitContext = split_context(context)\n",
    "print(\"lenSplit: \", len(splitContext[0]))\n",
    "print(len(splitContext))\n",
    "\n",
    "filteredContext = filter_error_chunks(splitContext)\n",
    "print(\"filteredContext length: \" , len(filteredContext))\n",
    "for i in range(len(filteredContext)):\n",
    "    print(filteredContext[i])\n",
    "    print(10*'-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(model, tokenizer, context, question):\n",
    "    \"\"\"\n",
    "    The function answers questions given context and question.\n",
    "    \n",
    "    model: This is the model that you're using to generate answers to the questions. \n",
    "    It could be any model that's capable of question answering, such as a transformer model.\n",
    "\n",
    "    param  tokenizer: This is the tokenizer that corresponds to your model. \n",
    "    It's used to convert your text data into a format that the model can understand.\n",
    "\n",
    "    param context: This is the text that the model will look at to find an answer to the question.\n",
    "\n",
    "    param question: This is the question that you're asking the model. \n",
    "    The model will generate an answer to this question based on the context.\n",
    "\n",
    "    The function returns an answer to the question based on the context. \n",
    "    The answer is generated by finding the tokens with the highest start and end scores, \n",
    "    and joining them together. If the end score is higher than the start score, \n",
    "    they are swapped to ensure the answer makes sense.\n",
    "    \"\"\"\n",
    "    # Encode the context and question\n",
    "    encoded = tokenizer.encode_plus(question, context, truncation=True, padding='max_length', max_length=512, return_tensors='pt')\n",
    "\n",
    "    # Get the start and end scores for all tokens\n",
    "    result = model(**encoded)\n",
    "    start_scores = result[\"start_logits\"]\n",
    "    end_scores = result[\"end_logits\"]\n",
    "\n",
    "    # Find the tokens with the highest start and end scores\n",
    "    answer_start = torch.argmax(start_scores)\n",
    "    answer_end = torch.argmax(end_scores)\n",
    "\n",
    "    # If the end score is higher than the start score, swap them\n",
    "    if answer_end < answer_start:\n",
    "        answer_start, answer_end = answer_end, answer_start\n",
    "\n",
    "    # Get the tokens for the answer\n",
    "    all_tokens = tokenizer.convert_ids_to_tokens(encoded['input_ids'][0])\n",
    "    answer = ' '.join(all_tokens[answer_start : answer_end+1])\n",
    "\n",
    "    return answer\n",
    "\n",
    "\n",
    "def vectorize_text(model, tokenizer, input_string):\n",
    "    \"\"\"\n",
    "    Vectorize a given input string.\n",
    "    \n",
    "    param model: This is the model used to encode the input string and get the output. \n",
    "    It could be any model that's capable of encoding text, such as a transformer model.\n",
    "\n",
    "    param tokenizer: This is the tokenizer that corresponds to your model. \n",
    "    It's used to convert your text data into a format that the model can understand.\n",
    "\n",
    "    param input_string: This is the text that you want to vectorize. \n",
    "    The function will convert this text into a numerical representation that \n",
    "    can be processed by the machine learning model.\n",
    "\n",
    "    The function returns a vector representation of the input string. \n",
    "    This vector is obtained by averaging the embeddings from the last hidden \n",
    "    state of the model's output.\n",
    "    \"\"\"\n",
    "    # Encode the input string\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        input_string,\n",
    "        add_special_tokens=True,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "    # Get the output from the model\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    # Get the embeddings from the last hidden state\n",
    "    embeddings = outputs.last_hidden_state\n",
    "\n",
    "    # Average the embeddings\n",
    "    vector = torch.mean(embeddings, dim=1)\n",
    "\n",
    "    # Convert tensor to numpy array\n",
    "    vector = vector.detach().numpy()\n",
    "\n",
    "    return vector\n",
    "\n",
    "\n",
    "def calculate_similarity(question_vector, answer_vector):\n",
    "    \"\"\"Calculate the cosine similarity between the question and answer vectors.\n",
    "    \n",
    "    param question_vector: This is the vector representation of the question. \n",
    "    It's obtained by transforming the question text into numerical data that \n",
    "    can be processed by the machine learning model.\n",
    "\n",
    "    param answer_vector: This is the vector representation of the answer. \n",
    "    It's obtained by transforming the answer text into numerical data that \n",
    "    can be processed by the machine learning model.\n",
    "\n",
    "    The function calculates and returns the cosine similarity between the \n",
    "    question and answer vectors. Cosine similarity is a measure of similarity \n",
    "    between two non-zero vectors of an inner product space that measures the \n",
    "    cosine of the angle between them. The closer the cosine similarity to 1, \n",
    "    the more similar the question and answer are.\n",
    "    \"\"\"\n",
    "\n",
    "    similarity = 1 - cosine(question_vector[0], answer_vector[0])\n",
    "\n",
    "    return similarity\n",
    "\n",
    "\n",
    "def find_best_answer(model, tokenizer, context, question, model_vec, num_answers=3, overlap=50, max_length=512):\n",
    "    \"\"\"Find the best answers to the question given a long context\n",
    "    param model: This is the model that you're using to generate answers to the questions. \n",
    "    It could be any model that's capable of question answering, such as a transformer model.\n",
    "\n",
    "    param tokenizer: This is the tokenizer that corresponds to your model.\n",
    "    It's used to convert your text data into a format that the model can understand.\n",
    "\n",
    "    param context: This is the text that the model will look at to find an answer to the question. \n",
    "    In this case, it's a long text that's split into chunks.\n",
    "\n",
    "    param question: This is the question that you're asking the model. \n",
    "    The model will generate an answer to this question based on the context.\n",
    "\n",
    "    param model_vec: This is a model used to vectorize the text, \n",
    "    i.e., convert the text into numerical data that can be processed by the machine learning model.\n",
    "\n",
    "    param num_answers (default=3): This is the number of best answers the function will return.\n",
    "\n",
    "    param overlap (default=50): This is the number of overlapping words between \n",
    "    two consecutive chunks when the context is split into chunks.\n",
    "\n",
    "    param max_length (default=512): This is the maximum length of each chunk. \n",
    "    The context is split into chunks of this length.\n",
    "\n",
    "    The function returns a list of tuples, where each tuple contains an answer \n",
    "    and its similarity score. The list is sorted in ascending order of similarity, \n",
    "    so the first element of the list is the answer with the lowest similarity, \n",
    "    and the last element is the answer with the highest similarity.\n",
    "        \n",
    "    \"\"\"\n",
    "    # Vectorize the question\n",
    "    question_vector = vectorize_text(model_vec, tokenizer, question)\n",
    "    \n",
    "    # Initialize the best answers and their similarities to the question\n",
    "    best_answers = [(None, -1) for _ in range(num_answers)]\n",
    "    \n",
    "    # Split the context into chunks\n",
    "    chunks = split_context(context, overlap, max_length)\n",
    "\n",
    "    filteredChunks = filter_error_chunks(chunks)\n",
    "    print(\"chunks info\")\n",
    "    print(\"lenght: \", len(chunks))\n",
    "    print(\"max chunk length: \" , len(max(chunks, key=len)))\n",
    "    print(100*'-')\n",
    "    print(\"filtered chunks info\")\n",
    "    print(\"lenght: \", len(filteredChunks))\n",
    "    print(\"max chunk length: \" , len(max(filteredChunks, key=len)))\n",
    "    \n",
    "    \n",
    "    # Generate an answer for each chunk and update the best answers if necessary\n",
    "    for chunk in filteredChunks:\n",
    "        print(\"current chunk: \")\n",
    "        print(chunk)\n",
    "        print(100*'-')\n",
    "        answer = answer_question(model, tokenizer, chunk, question)\n",
    "        if answer is not None:\n",
    "            answer_vector = vectorize_text(model_vec, tokenizer, answer)\n",
    "            if answer_vector is not None:\n",
    "                similarity = calculate_similarity(question_vector, answer_vector)\n",
    "                # Check if the similarity is higher than the current lowest in best_answers\n",
    "                if similarity > best_answers[0][1]:\n",
    "                    # Replace the lowest\n",
    "                    best_answers[0] = (answer, similarity)\n",
    "                    # Sort the list so the lowest similarity is first\n",
    "                    best_answers = sorted(best_answers, key=lambda x: x[1])\n",
    "    # Return the answers along with their similarities\n",
    "    return best_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertModel: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, AutoModel\n",
    "\n",
    "model_name = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name) #used for answering\n",
    "model_vec = AutoModel.from_pretrained(model_name) #used for vectorization\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name) #used for tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linesUnder513 1000\n",
      "maxLineLength:  Nov 09 13:11:48 CMX50070-101776 rs_callysto.xlapi_nb_container_settings[3036]: [info     ] Write .env file for XlapiContainerSettings(ip='*', port=7777, base_url='/notebooks', token='instrument', xlapi_instrument_address='localhost') to /run/user/0/mrt.callysto/.env.prod context=_setup | start pathname=/usr/lib/callysto/venv/lib/python3.10/site-packages/rs_callysto/xlapi_nb_container_settings.py lineno=46\n",
      "chunks info\n",
      "lenght:  301\n",
      "max chunk length:  511\n",
      "----------------------------------------------------------------------------------------------------\n",
      "filtered chunks info\n",
      "lenght:  13\n",
      "max chunk length:  508\n",
      "current chunk: \n",
      "Nov 09 13:11:42 CMX50070-101776 systemd[1859]: mrt.base.service: Executable /usr/local/LoggingService/bin/stopcurrentlogger.sh missing, skipping: Permission denied\n",
      "Nov 09 13:11:42 CMX50070-101776 systemd[1]: Started MRT Base Software.\n",
      "Nov 09 13:11:42 CMX50070-101776 systemd[1]: Started Battery Measurement Service.\n",
      "Nov 09 13:11:42 CMX50070-101776 systemd[1]: Starting MRT Deployment Calculation Service...\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "current chunk: \n",
      "Nov 09 13:11:42 CMX50070-101776 Xserver[1884]:         Before reporting problems, check http://wiki.x.org\n",
      "Nov 09 13:11:42 CMX50070-101776 Xserver[1884]:         to make sure that you have the latest version.\n",
      "Nov 09 13:11:42 CMX50070-101776 Xserver[1884]: Markers: (--) probed, (**) from config file, (==) default setting,\n",
      "Nov 09 13:11:42 CMX50070-101776 Xserver[1884]:         (++) from command line, (!!) notice, (II) informational,\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "current chunk: \n",
      "Nov 09 13:11:42 CMX50070-101776 Xserver[1884]:         (WW) warning, (EE) error, (NI) not implemented, (??) unknown.\n",
      "Nov 09 13:11:42 CMX50070-101776 Xserver[1884]: (==) Log file: \"/var/log/Xorg.0.log\", Time: Thu Nov  9 13:11:42 2023\n",
      "Nov 09 13:11:42 CMX50070-101776 Xserver[1884]: (==) Using config directory: \"/etc/X11/xorg.conf.d\"\n",
      "Nov 09 13:11:42 CMX50070-101776 Xserver[1884]: (==) Using system config directory \"/usr/share/X11/xorg.conf.d\"\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "current chunk: \n",
      "Nov 09 13:11:43 CMX50070-101776 sshd[2446]: error: kex_exchange_identification: Connection closed by remote host\n",
      "Nov 09 13:11:43 CMX50070-101776 sshd[2446]: Connection closed by 10.0.54.73 port 48154\n",
      "Nov 09 13:11:43 CMX50070-101776 systemd[1]: sshd@0-10.102.37.173:22-10.0.54.73:48154.service: Deactivated successfully.\n",
      "Nov 09 13:11:43 CMX50070-101776 node[405]: Created application facet succesfully\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "current chunk: \n",
      "Nov 09 13:11:43 CMX50070-101776 systemd[1]: Started OpenSSH Per-Connection Daemon (172.24.26.36:58080).\n",
      "Nov 09 13:11:43 CMX50070-101776 sshd[2542]: error: kex_exchange_identification: Connection closed by remote host\n",
      "Nov 09 13:11:43 CMX50070-101776 sshd[2542]: Connection closed by 172.24.26.36 port 58080\n",
      "Nov 09 13:11:43 CMX50070-101776 systemd[1]: sshd@1-10.102.37.173:22-172.24.26.36:58080.service: Deactivated successfully.\n",
      "Nov 09 13:11:43 CMX50070-101776 node[405]: Get index for mrt.nrmmwue-helpk70...\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "current chunk: \n",
      "Nov 09 13:11:44 CMX50070-101776 dockerd[1853]: time=\"2023-11-09T13:11:44.902972912Z\" level=info msg=\"[graphdriver] using prior storage driver: overlay2\"\n",
      "Nov 09 13:11:44 CMX50070-101776 Xserver[2286]: libGL error: MESA-LOADER: failed to open radeonsi: /usr/lib/dri/radeonsi_dri.so: cannot open shared object file: No such file or directory (search paths /usr/lib/dri, suffix _dri)\n",
      "Nov 09 13:11:44 CMX50070-101776 Xserver[2286]: libGL error: failed to load driver: radeonsi\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "current chunk: \n",
      "Nov 09 13:11:44 CMX50070-101776 kernel: CMX_AU_IOCTL_SET_CLEAR_SIG_FPGA call\n",
      "Nov 09 13:11:44 CMX50070-101776 kernel: CMX_IOCTL_GET_SLOT_ID call\n",
      "Nov 09 13:11:44 CMX50070-101776 Xserver[2286]: libGL error: failed to open /dev/dri/card0: No such file or directory\n",
      "Nov 09 13:11:44 CMX50070-101776 Xserver[2286]: libGL error: failed to load driver: radeonsi\n",
      "Nov 09 13:11:44 CMX50070-101776 rngd[407]: [jitter]: Enabling JITTER rng support\n",
      "Nov 09 13:11:44 CMX50070-101776 rngd[407]: [jitter]: Initialized\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "current chunk: \n",
      "Nov 09 13:11:45 CMX50070-101776 dockerd[1853]: time=\"2023-11-09T13:11:45.067562294Z\" level=warning msg=\"Your kernel does not support cgroup blkio weight\"\n",
      "Nov 09 13:11:45 CMX50070-101776 dockerd[1853]: time=\"2023-11-09T13:11:45.067596248Z\" level=warning msg=\"Your kernel does not support cgroup blkio weight_device\"\n",
      "Nov 09 13:11:45 CMX50070-101776 dockerd[1853]: time=\"2023-11-09T13:11:45.067605265Z\" level=warning msg=\"Your kernel does not support cgroup blkio throttle.read_bps_device\"\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "current chunk: \n",
      "Nov 09 13:11:45 CMX50070-101776 dockerd[1853]: time=\"2023-11-09T13:11:45.067612849Z\" level=warning msg=\"Your kernel does not support cgroup blkio throttle.write_bps_device\"\n",
      "Nov 09 13:11:45 CMX50070-101776 dockerd[1853]: time=\"2023-11-09T13:11:45.067621135Z\" level=warning msg=\"Your kernel does not support cgroup blkio throttle.read_iops_device\"\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "current chunk: \n",
      "Nov 09 13:11:45 CMX50070-101776 dockerd[1853]: time=\"2023-11-09T13:11:45.067628810Z\" level=warning msg=\"Your kernel does not support cgroup blkio throttle.write_iops_device\"\n",
      "Nov 09 13:11:45 CMX50070-101776 dockerd[1853]: time=\"2023-11-09T13:11:45.067827022Z\" level=info msg=\"Loading containers: start.\"\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "current chunk: \n",
      "Nov 09 13:11:45 CMX50070-101776 audit: PROCTITLE proctitle=2F7573722F7362696E2F69707461626C6573002D2D77616974002D74006E6174002D4900504F5354524F5554494E47002D73003130302E38322E36342E302F32350000002D6F00646F636B657230002D6A004D415351554552414445\n",
      "Nov 09 13:11:45 CMX50070-101776 Xserver[2286]: libEGL warning: DRI2: could not open /dev/dri/card0 (No such file or directory)\n",
      "Nov 09 13:11:45 CMX50070-101776 audit[2747]: NETFILTER_CFG table=nat family=2 entries=8 op=xt_replace pid=2747 comm=\"iptables\"\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "current chunk: \n",
      "Nov 09 13:11:45 CMX50070-101776 xu_launcher[2941]: ./o_upper_20.bmp\n",
      "Nov 09 13:11:45 CMX50070-101776 xu_launcher[2941]: ./six_12.bmp\n",
      "Nov 09 13:11:45 CMX50070-101776 xu_launcher[2941]: ./g_lower_12.bmp\n",
      "Nov 09 13:11:45 CMX50070-101776 xu_launcher[2941]: ./warning_mark.bmp\n",
      "Nov 09 13:11:45 CMX50070-101776 xu_launcher[2941]: ./t_lower_12.bmp\n",
      "Nov 09 13:11:45 CMX50070-101776 avahi-daemon[379]: Interface docker0.IPv4 no longer relevant for mDNS.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "current chunk: \n",
      "Nov 09 13:11:48 CMX50070-101776 rs_callysto.callysto_server._logging_configuration[3036]: [info     ] write json log to /var/log/callysto.log.json type=json log_level=20 filename=/var/log/callysto.log.json pathname=/usr/lib/callysto/venv/lib/python3.10/site-packages/rs_callysto/callysto_server/_logging_configuration.py lineno=183\n",
      "Nov 09 13:11:48 CMX50070-101776 bash[3046]: Removing network cmxreporting_default\n",
      "Nov 09 13:11:48 CMX50070-101776 bash[3046]: Network cmxreporting_default not found.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The best answer is: [('failed to load driver : ra ##de ##ons ##i', 0.7873050570487976), ('failed to load driver : ra ##de ##ons ##i', 0.7873050570487976), ('dr ##i ##2 : could not open / dev / dr ##i / card ##0', 0.7949483394622803)]\n"
     ]
    }
   ],
   "source": [
    "#question = \"What error message is displayed?\"\n",
    "question = \"What is causing the error?\"\n",
    "\n",
    "# Find the best answer to the question given the context\n",
    "best_answer = find_best_answer(model, tokenizer, contextLines, question, model_vec, num_answers=3, overlap=50, max_length=tokenizer.model_max_length)\n",
    "print(f\"The best answer is: {best_answer}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackaTUM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
